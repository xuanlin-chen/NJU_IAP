# 南京大学信息聚合平台（NJU Information Aggregation Platform)
懒得写readme初稿了，用cac的后面再根据需求和进展改吧

面向本科生的信息渠道太多了，如果不能及时关注所有的渠道，可能会出现信息错漏的情况

解决这个问题的正向思路是搭建一个信息聚集平台，尽可能的要求各部门、老师都将面向本科生的信息发布在这个平台上，显然这种设想的可行性很低

还有一种思路是将信息聚集在某个平台上，让本科生集中到这个平台去查看所有的信息，这种方法的可行性变高了，但是信息聚集这件事需要耗费大量的人力（以前民间有过汇总讲座的团队，在各学院寻找志愿者统计汇总讲座信息）

我们需要开发一种技术手段，目的是通过降低聚集信息的人工成本，来使得更多的人加入到这个信息聚合平台的建设中

![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/294617/1743416524505-6616c407-26e9-461d-9a93-9f6d49da57ae.jpeg)

可以参考的技术路径：

1. 分类器，调用大模型的能力对用户输入的内容进行分类判定
2. 针对一个确定的分类，使用个性化的Prompt或其他人工智能技术进行后续处理，处理的目标是提取关键信息，形成结构化的数据
    1. 海报照片：提取时间、面向对象、地点、内容、参与条件等信息
    2. 通知消息：提取时间节点、通知对象、内容等信息
    3. 规章制度：提取适用范围、时间节点、具体内容等信息
    4. 公示公告：提取涉及对象、告知事项等信息
    5. 网页新闻：提取时间节点、涉及对象、具体内容等信息
    6. 公众号文章：提取时间节点、涉及对象、具体内容等信息
    7. 内网穿透：研究自动登录并截图保存页面的自动化智能脚本
    8. 微信、QQ：研究聊天记录自动导出工具或者后台自动截图脚本
3. 对这些信息进行储存管理，搭建数据库或者图数据库
4. 调用大模型对数据进行处理，实现索引或者消息提醒的功能

---

事实上，我们可以获取的数据源是很广的，结构化数据的要点在于锚定一些关键信息：

+ 时间：什么时候开始、什么时候截止
+ 对象：谁可以参加、谁必须参加、有谁参加
+ 内容：保留原链接，让大模型判断用户是否需要/感兴趣，让读者自己判断

